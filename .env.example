# LLM Provider (openai | ollama)
LLM_PROVIDER=ollama

# Environment (development | production)
ENVIRONMENT=development

# Ollama (Development - Local Docker)
OLLAMA_BASE_URL=http://localhost:11434
# Recommended models for local development (less memory):
# - tinyllama (~1GB RAM) - Fast, minimal quality
# - llama2 (~4GB RAM) - Good balance
# - phi3:mini (~3GB RAM) - Good quality, moderate speed
# - gpt-oss:20b (~13GB RAM) - Best quality, requires 16GB+ Docker memory
OLLAMA_MODEL=tinyllama
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# OpenAI (Production)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=your_api_key_here
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# ChromaDB
CHROMA_PERSIST_DIRECTORY=./data/chroma_db
